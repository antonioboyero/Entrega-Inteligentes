Información del rescate:
F: Free (libre)
S: Start (inicio)
X: Blocked (bloqueado)
T: Trapped (atrapado)
P: Danger (peligro)
D: Fatal Danger (peligro fatal)

Plano del rescate:

T F F S F F T
F P F X F F D


Filas: 2, Columnas: 7
Bloqueos: [[1, 3]]
Punto de partida: (0, 3)
Peligros: [(1, 1)]
Personas atrapadas: {(0, 0): 1, (0, 6): 5}
Peligros fatales: {(1, 6): -7}

---INFO

gamma = 0.9
threshold = 0.01

---EMPEZAMOS

Política inicial:
Estado Position: (0, 1): Acción Action: LEFT
Estado Position: (0, 2): Acción Action: DOWN
Estado Position: (0, 3): Acción Action: RIGHT
Estado Position: (0, 4): Acción Action: LEFT
Estado Position: (0, 5): Acción Action: RIGHT
Estado Position: (1, 0): Acción Action: UP
Estado Position: (1, 1): Acción Action: UP
Estado Position: (1, 2): Acción Action: LEFT
Estado Position: (1, 4): Acción Action: RIGHT
Estado Position: (1, 5): Acción Action: UP
It: 

Iteración 1
Estado: Position: (0, 1), Acción: Action: LEFT
    Transición a Position: (0, 2) con probabilidad 0.09999999999999998, Utilidad de Position: (0, 2): 0
    Transición a Position: (1, 1) con probabilidad 0.09999999999999998, Utilidad de Position: (1, 1): 0
    Transición a Position: (0, 0) con probabilidad 0.8, Utilidad de Position: (0, 0): 1
Recompensa: -1, Suma esperada: 0.8, Nueva utilidad: -0.2799999999999999
Estado: Position: (0, 2), Acción: Action: DOWN
    Transición a Position: (0, 3) con probabilidad 0.09999999999999998, Utilidad de Position: (0, 3): 0
    Transición a Position: (1, 2) con probabilidad 0.8, Utilidad de Position: (1, 2): 0
    Transición a Position: (0, 1) con probabilidad 0.09999999999999998, Utilidad de Position: (0, 1): 0
Recompensa: -1, Suma esperada: 0.0, Nueva utilidad: -1.0
Estado: Position: (0, 3), Acción: Action: RIGHT
    Transición a Position: (0, 4) con probabilidad 0.8, Utilidad de Position: (0, 4): 0
    Transición a Position: (0, 2) con probabilidad 0.19999999999999996, Utilidad de Position: (0, 2): 0
Recompensa: -1, Suma esperada: 0.0, Nueva utilidad: -1.0
Estado: Position: (0, 4), Acción: Action: LEFT
    Transición a Position: (0, 5) con probabilidad 0.09999999999999998, Utilidad de Position: (0, 5): 0
    Transición a Position: (1, 4) con probabilidad 0.09999999999999998, Utilidad de Position: (1, 4): 0
    Transición a Position: (0, 3) con probabilidad 0.8, Utilidad de Position: (0, 3): 0
Recompensa: -1, Suma esperada: 0.0, Nueva utilidad: -1.0
Estado: Position: (0, 5), Acción: Action: RIGHT
    Transición a Position: (0, 6) con probabilidad 0.8, Utilidad de Position: (0, 6): 5
    Transición a Position: (1, 5) con probabilidad 0.09999999999999998, Utilidad de Position: (1, 5): 0
    Transición a Position: (0, 4) con probabilidad 0.09999999999999998, Utilidad de Position: (0, 4): 0
Recompensa: -1, Suma esperada: 4.0, Nueva utilidad: 2.6
Estado: Position: (1, 0), Acción: Action: UP
    Transición a Position: (0, 0) con probabilidad 0.8, Utilidad de Position: (0, 0): 1
    Transición a Position: (1, 1) con probabilidad 0.19999999999999996, Utilidad de Position: (1, 1): 0
Recompensa: -1, Suma esperada: 0.8, Nueva utilidad: -0.2799999999999999




ASI CONTINUARIAMOS HASTA COMPLETAR TODOS LOS ESTADOS

CALCULARIAMOS LAS SIGUIENTES UTILIDADES EN BASE A LAS CALCULADAS EN LA ITERACION ANTERIOR




Estado: Position: (0, 1), Acción: Action: LEFT
    Transición a Position: (0, 2) con probabilidad 0.09999999999999998, Utilidad de Position: (0, 2): -1.0
    Transición a Position: (1, 1) con probabilidad 0.09999999999999998, Utilidad de Position: (1, 1): -5.0
    Transición a Position: (0, 0) con probabilidad 0.8, Utilidad de Position: (0, 0): 1
Recompensa: -1, Suma esperada: 0.20000000000000018, Nueva utilidad: -0.8199999999999998
Estado: Position: (0, 2), Acción: Action: DOWN
    Transición a Position: (0, 3) con probabilidad 0.09999999999999998, Utilidad de Position: (0, 3): -1.0
    Transición a Position: (1, 2) con probabilidad 0.8, Utilidad de Position: (1, 2): -1.0
    Transición a Position: (0, 1) con probabilidad 0.09999999999999998, Utilidad de Position: (0, 1): -0.2799999999999999
Recompensa: -1, Suma esperada: -0.928, Nueva utilidad: -1.8352
Estado: Position: (0, 3), Acción: Action: RIGHT
    Transición a Position: (0, 4) con probabilidad 0.8, Utilidad de Position: (0, 4): -1.0
    Transición a Position: (0, 2) con probabilidad 0.19999999999999996, Utilidad de Position: (0, 2): -1.0
Recompensa: -1, Suma esperada: -1.0, Nueva utilidad: -1.9
Estado: Position: (0, 4), Acción: Action: LEFT
    Transición a Position: (0, 5) con probabilidad 0.09999999999999998, Utilidad de Position: (0, 5): 2.6
    Transición a Position: (1, 4) con probabilidad 0.09999999999999998, Utilidad de Position: (1, 4): -1.0
    Transición a Position: (0, 3) con probabilidad 0.8, Utilidad de Position: (0, 3): -1.0
Recompensa: -1, Suma esperada: -0.6400000000000001, Nueva utilidad: -1.576


...HEMOS SUPRIMIDO TODOS LO DEMÁS CALCULOS AQUI SUPONEMOS QUE HAN FINALIZADO

---AQUI COMPROBAMOS QUE SE REALIZA BIEN LA MEJORA DE LA POLÍTICA SELECCIONADO LA ACCIÓN CON MAYOR Utilidad
---DEJAMOS UN PAR DE CASOS PARA QUE SE VEAN CORRECTOS

Estado Position: (0, 1):
  Acción Action: RIGHT: Utilidad esperada -5.910171077222347
  Acción Action: DOWN: Utilidad esperada -6.025246877347434
  Acción Action: LEFT: Utilidad esperada -0.5483797727299751
  Mejor acción seleccionada: Action: LEFT con utilidad -0.5483797727299751

Estado Position: (0, 2):
  Acción Action: RIGHT: Utilidad esperada -5.096953935994368
  Acción Action: DOWN: Utilidad esperada -6.316390578198502
  Acción Action: LEFT: Utilidad esperada -2.4150977714126753
  Mejor acción seleccionada: Action: LEFT con utilidad -2.4150977714126753

Estado Position: (0, 3):
  Acción Action: RIGHT: Utilidad esperada -5.108392431384044
  Acción Action: LEFT: Utilidad esperada -6.271874505516409
  Mejor acción seleccionada: Action: RIGHT con utilidad -5.108392431384044


Política INICIAL:
Estado Position: (0, 1): Acción Action: LEFT
Estado Position: (0, 2): Acción Action: DOWN
Estado Position: (0, 3): Acción Action: RIGHT
Estado Position: (0, 4): Acción Action: LEFT
Estado Position: (0, 5): Acción Action: RIGHT
Estado Position: (1, 0): Acción Action: UP
Estado Position: (1, 1): Acción Action: UP
Estado Position: (1, 2): Acción Action: LEFT
Estado Position: (1, 4): Acción Action: RIGHT
Estado Position: (1, 5): Acción Action: UP

Política mejorada:
Estado Position: (0, 1): Acción Action: LEFT --VEMOS COMO HA SELECCIONADO LAS DE MAYOR UTILIDAD
Estado Position: (0, 2): Acción Action: LEFT --VEMOS COMO HA SELECCIONADO LAS DE MAYOR UTILIDAD
Estado Position: (0, 3): Acción Action: RIGHT
Estado Position: (0, 4): Acción Action: RIGHT
Estado Position: (0, 5): Acción Action: RIGHT
Estado Position: (1, 0): Acción Action: UP
Estado Position: (1, 1): Acción Action: UP
Estado Position: (1, 2): Acción Action: UP
Estado Position: (1, 4): Acción Action: RIGHT
Estado Position: (1, 5): Acción Action: UP


--ASI SEGUIRIAMOS HASTA COMPLETAR LAS ITERACIONES O CUANDO CONVERGE EL ALGORITMO

Iteración 2

Política anterior:
Estado Position: (0, 1): Acción Action: LEFT
Estado Position: (0, 2): Acción Action: LEFT
Estado Position: (0, 3): Acción Action: RIGHT
Estado Position: (0, 4): Acción Action: RIGHT
Estado Position: (0, 5): Acción Action: RIGHT
Estado Position: (1, 0): Acción Action: UP
Estado Position: (1, 1): Acción Action: UP
Estado Position: (1, 2): Acción Action: UP
Estado Position: (1, 4): Acción Action: RIGHT
Estado Position: (1, 5): Acción Action: UP

Política mejorada:
Estado Position: (0, 1): Acción Action: LEFT
Estado Position: (0, 2): Acción Action: RIGHT
Estado Position: (0, 3): Acción Action: RIGHT
Estado Position: (0, 4): Acción Action: RIGHT
Estado Position: (0, 5): Acción Action: RIGHT
Estado Position: (1, 0): Acción Action: UP
Estado Position: (1, 1): Acción Action: UP
Estado Position: (1, 2): Acción Action: UP
Estado Position: (1, 4): Acción Action: UP
Estado Position: (1, 5): Acción Action: UP

Se alcanzó el número máximo de iteraciones que fueron fijadas a 2.

Política final:
Estado Position: (0, 1): Acción Action: LEFT
Estado Position: (0, 2): Acción Action: RIGHT
Estado Position: (0, 3): Acción Action: RIGHT
Estado Position: (0, 4): Acción Action: RIGHT
Estado Position: (0, 5): Acción Action: RIGHT
Estado Position: (1, 0): Acción Action: UP
Estado Position: (1, 1): Acción Action: UP
Estado Position: (1, 2): Acción Action: UP
Estado Position: (1, 4): Acción Action: UP
Estado Position: (1, 5): Acción Action: UP

Política óptima calculada después de iteración de políticas.
* ← → → → → *
↑ ↑ ↑ X ↑ ↑ .

Iteraciones realizadas: 2 